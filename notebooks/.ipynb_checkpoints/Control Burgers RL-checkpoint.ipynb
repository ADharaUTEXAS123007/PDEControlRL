{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlling Burgers' Equation with Reinforcement Learning\n",
    "This notebook will demonstrate how reinforcement learning can be used to control Burgers' equation, a nonlinear PDE. The approach uses the reinforcement learning framework [stable_baselines3](https://github.com/DLR-RM/stable-baselines3) and the differentiable PDE solver [Î¦<sub>Flow</sub>](https://github.com/tum-pbs/PhiFlow). As a reinforcement learning algorithm, [PPO](https://arxiv.org/abs/1707.06347v2) was selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/anaconda3/envs/phiflow/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "/home/felix/anaconda3/envs/phiflow/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "/home/felix/anaconda3/envs/phiflow/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "/home/felix/anaconda3/envs/phiflow/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "/home/felix/anaconda3/envs/phiflow/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "/home/felix/anaconda3/envs/phiflow/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys; sys.path.append('../src'); sys.path.append('../PDE-Control/PhiFlow'); sys.path.append('../PDE-Control/src')\n",
    "from phi.flow import *\n",
    "import matplotlib.pyplot as plt\n",
    "import burgers_plots as bplt\n",
    "from experiment import BurgersTraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = Domain([32], box=box[0:1]) # 1d grid resolution and physical size\n",
    "viscosity = 0.003 # viscosity constant for Burgers' equation\n",
    "step_count = 32 # length of each trajectory\n",
    "dt = 0.03 # time step size\n",
    "diffusion_substeps = 1 # how many diffusion steps to perform at each solver step\n",
    "\n",
    "n_envs = 10 # On how many environments to train in parallel, load balancing\n",
    "final_reward_factor = step_count # How hard to punish the agent for not reaching the goal if that is the case\n",
    "steps_per_rollout = step_count * 10 # How many steps to collect per environment between agent updates\n",
    "training_timesteps = steps_per_rollout * 1000 # How long the actual training should be\n",
    "n_epochs = 10 # How many epochs to perform during agent update\n",
    "learning_rate = 1e-4 # Learning rate for agent updates\n",
    "batch_size = 128 # Batch size for agent updates\n",
    "test_path = 'forced-burgers-clash' # Path of the used test set for comparison to cfe method\n",
    "test_range = range(100) # Test samples inside the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start training, we create a trainer object, which manages the environment and the agent internally. Additionally, a directory for storing models, logs, and hyperparameters is created. This way, training can be continued at any later point using the same configuration. If the model folder specified in `exp_name` already exists, the agent within is loaded. Otherwise, a new agent is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = BurgersTraining(\n",
    "    exp_name='../networks/rl-models/ControlBurgersBench',\n",
    "    domain=domain,\n",
    "    viscosity=viscosity,\n",
    "    step_count=step_count,\n",
    "    dt=dt,\n",
    "    diffusion_substeps=diffusion_substeps,\n",
    "    n_envs=n_envs,\n",
    "    final_reward_factor=final_reward_factor,\n",
    "    steps_per_rollout=steps_per_rollout,\n",
    "    n_epochs=n_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=batch_size,\n",
    "    test_path=test_path,\n",
    "    test_range=range(100),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are set up to start training the agent. The next line will take quite some time to execute, so grab a coffee or take your dog for a walk or so.\n",
    "\n",
    "`n_rollouts` denotes the length of the training\n",
    "\n",
    "`save_freq` specifies the number of epochs after which the stored model is overwritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train(n_rollouts=1000, save_freq=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the reward evolved during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to open the tensorboard logs. If necessary, replace the path with the one leading to the log folder of your experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir rl-models/ControlBurgersBench3/tensorboard-log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at how the agent is performing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = trainer.env\n",
    "\n",
    "obs = env.reset()\n",
    "bplt.burgers_figure('RL Reconstruction')\n",
    "plt.plot(obs[0][:,0], color=bplt.gradient_color(0, step_count+1), linewidth=0.8)\n",
    "plt.legend(['Initial state in dark red, final state in dark blue,'])\n",
    "plt.ylim(-2, 2)\n",
    "for frame in range(1, step_count):\n",
    "    act = trainer.predict(obs, deterministic=True)\n",
    "    obs, _, _, _ = env.step(act)\n",
    "    plt.plot(obs[0][:,0], color=bplt.gradient_color(frame, step_count+1), linewidth=0.8)\n",
    "plt.plot(env.goal_state.velocity.data[0,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phiflow",
   "language": "python",
   "name": "phiflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
