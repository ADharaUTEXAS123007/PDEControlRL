-Begin with the previous scenario (target fields generated by applying
        the same random force value across the whole field at every
        time step)
    -Test out a fully convolutional network (u-net variant) for the actor
        => critic: same network but with a linear layer with one output
                at the end

Experiment 1: 
    -Forces and Proximity considered 1:1 in reward
    -Proximity taken into account at the end of the trajectory, then 
            multiplied by the number of timesteps (here: 32)
    -2x down-/upsampling in the u-net
    -Training for 400 epochs (previous experiments in this domain were 
            much longer, in the range of 1000-5000 epochs)
    -Gamma: 0.99, pi lr = 2e-4, vf lr = 1e-3

-Switch to the new scenario (forces being distributed in a gaussian curve
        across the field, same values at each time step)

Experiment 2:
    -Forces and Proximity considered 1:1 in reward
    -Proximity in the end of the trajectory, forces at each step
    -2x Down-/upsampling in the u-net
    -Training for 850 epochs
    -Gamma: 0.99, pi lr = 2e-4, vf lr = 1e-3
    
-Network did not generate forces strong enough to approximate target 
        field
    => Rebalance the factors in the reward
    
Experiment 3:
    -Forces not considered in reward, proximity at each timestep
    -2x down-/upsampling in the u-net
    -Training for 850 epochs
    -Gamma: 0.99, pi lr = 2e-4, vf lr = 1e-3
    
Experiment 4:
    -Forces and Proximity considered 1:4 in reward
    -Proximity in the end, forces at each step
    -2x down-/upsampling
    -Training for 1250 epochs
    -Gamma: 0.99, pi lr = 2e-4, vf lr = 1e-3
    
Experiment 5:
    -Forces and Proximity considered 2:25 in reward
    -Proximity in the end, forces at each step
    -2x down-/upsampling
    -Gamma: 0.99, pi lr = 2e-4, vf lr = 1e-3


