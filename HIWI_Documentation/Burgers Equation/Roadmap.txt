-Begin with the previous scenario (target fields generated by applying
        the same random force value across the whole field at every
        time step)
    -Test out a fully convolutional network (u-net variant) for the actor
        => critic: same network but with a linear layer with one output
                at the end

Experiment 1: 
    -Forces and Proximity considered 1:1 in reward
    -Proximity taken into account at the end of the trajectory, then 
            multiplied by the number of timesteps (here: 32)
    -2x down-/upsampling in the u-net
    -Training for 400 epochs (previous experiments in this domain were 
            much longer, in the range of 1000-5000 epochs)
    -Gamma: 0.99, pi lr = 2e-4, vf lr = 1e-3

-Switch to the new scenario (forces being distributed in a gaussian curve
        across the field, same values at each time step)

Experiment 2:
    -Forces and Proximity considered 1:1 in reward
    -Proximity in the end of the trajectory, forces at each step
    -2x Down-/upsampling in the u-net
    -Training for 850 epochs
    -Gamma: 0.99, pi lr = 2e-4, vf lr = 1e-3
    
-Network did not generate forces strong enough to approximate target 
        field
    => Rebalance the factors in the reward
    
Experiment 3:
    -Forces not considered in reward, proximity at each timestep
    -2x down-/upsampling in the u-net
    -Training for 850 epochs
    -Gamma: 0.99, pi lr = 2e-4, vf lr = 1e-3
    
Experiment 4:
    -Forces and Proximity considered 1:4 in reward
    -Proximity in the end, forces at each step
    -2x down-/upsampling
    -Training for 1250 epochs
    -Gamma: 0.99, pi lr = 2e-4, vf lr = 1e-3
    
Experiment 5:
    -Forces and Proximity considered 2:25 in reward
    -Proximity in the end, forces at each step
    -2x down-/upsampling
    -Gamma: 0.99, pi lr = 2e-4, vf lr = 1e-3

-'New' reward model: unify field proximity and force rewards
    => In the last time step, subtract the forces that would be necessary 
            to directly reach the target state from the reward
        => Order in last timestep:
            1) Apply network forces
            2) Perform simulation
            3) Apply necessary forces to reach target state
            => Reward in final timestep: 
                network_forces + x * remaining_forces
                x: weighting factor, for example equal to the epoch
                        length
        => Why could a weighting factor be necessary/a good idea?
            - Otherwise the network does not care as much if the
                    target state is reached after the simulation step
                    after its last action, if it would not output any
                    forces in the last step the reward would be similar
            -With the factor > 1: Network has a strong incentive to
                    minimize the remaining forces

Experiment 6:
    -weighting factor for remaining forces equal to epoch length
    -2x down-/upsampling
    -Gamma: 0.99, pi lr = 2e-4, vf lr = 1e-3
    
Experiment 7:
    -equivalent to experiment 6, but with the old experiment setup
            where forces were equal across the field
            
            
-Use Philipp's version of U-Net to make more expressive comparisons
        to the supervised method
-Allow to define different architectures for actor and critic networks
        
Experiment 8:
    -5 levels with 4, 8, 16, 16, 16 filters
    -Pi learning rates: 1e-3, 1e-4, 
    
Experiment 9:
    -Try to reduce the bottleneck
    -5 levels with 8, 8, 16, 16, 16 filters
    - Pi lr: 2e-4
    
Experiment 10:
    -New implementation that is actually working
    -Pi lr: 1e-5
    -Epoch length: 6400 steps
    -300 epochs
    
Experiment 11:
    -Pi lr: 1e-5
    -Epoch length: 3200 steps
    
    
